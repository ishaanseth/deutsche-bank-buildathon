{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1. Initial Setup and Dependencies\n",
        "This section installs required packages, imports necessary libraries, and sets up GPU support. It also handles Google Drive mounting and environment variable loading for secure credential management."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "N3DsXqQ05FNj",
        "outputId": "1c4f0c42-e2fb-44da-bc14-eba5c939227f"
      },
      "outputs": [],
      "source": [
        "# Colab-specific installations\n",
        "!pip install transformers sentencepiece pymongo bs4 requests python-dotenv\n",
        "!pip install \"pymongo[srv]\"\n",
        "\n",
        "import datetime\n",
        "import requests\n",
        "import os\n",
        "import torch\n",
        "from bs4 import BeautifulSoup\n",
        "from pymongo.mongo_client import MongoClient\n",
        "from pymongo.server_api import ServerApi\n",
        "from dotenv import load_dotenv\n",
        "from datetime import datetime, timedelta\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive to access or save files\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Hugging Face transformers\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "# Check GPU availability\n",
        "print(\"GPU Available:\", torch.cuda.is_available())\n",
        "print(\"GPU Device Name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\")\n",
        "\n",
        "# Load environment variables (you may need to create this file in your Google Drive)\n",
        "load_dotenv('/content/drive/MyDrive/cred.env')\n",
        "\n",
        "mongo_username = 'hidden' # Replace with your MongoDB username\n",
        "mongo_password = 'hidden' # Replace with your MongoDB password"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. MongoDB Connection\n",
        "Establishes connection to MongoDB database using credentials and creates a client instance for database operations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "-HMY5RSxYRhP",
        "outputId": "50f9e0b7-bde7-4d23-c605-15d6089d25bd"
      },
      "outputs": [],
      "source": [
        "# MongoDB setup\n",
        "uri = 'mongodb+srv://hidden.km1fx.mongodb.net/?retryWrites=true&w=majority&appName=buildathon' # Replace with your MongoDB URI\n",
        "\n",
        "client = MongoClient(uri, server_api=ServerApi('1'))\n",
        "\n",
        "# Send a ping to confirm a successful connection\n",
        "try:\n",
        "    client.admin.command('ping')\n",
        "    print(\"Pinged your deployment. You successfully connected to MongoDB!\")\n",
        "except Exception as e:\n",
        "    print(e)\n",
        "\n",
        "db = client['news_scraper']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3. Model Initialization\n",
        "Sets up the NLP models for text summarization and sentiment analysis using BART and transformers library. Configures GPU acceleration for better performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LlSDIOT5YS3Y"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Use an open-source model for summarization, optimized for T4 GPU\n",
        "model_name = \"facebook/bart-large-cnn\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to('cuda')\n",
        "summarizer = pipeline(\"summarization\", model=model, tokenizer=tokenizer, device=0)  # Use GPU\n",
        "\n",
        "# Define the custom prompt template\n",
        "prompt_template = '''Summarize the following news article in a concise paragraph, focusing on the main highlights.\n",
        "Remove any text not directly related to the article content. Ensure the summary includes the publishing date if available.\n",
        "Article text: {article}'''\n",
        "\n",
        "# Instantiate the sentiment analysis pipeline\n",
        "sentiment_analyzer = pipeline(\"sentiment-analysis\", device=0)  # Use GPU\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 4. Web Scraping Function\n",
        "Defines utility function to extract text content from web URLs using BeautifulSoup library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hHmFWMucYU-a"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Function to get text from URL using BeautifulSoup\n",
        "def get_text_from_url(url):\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        html = response.text\n",
        "        soup = BeautifulSoup(html, 'html.parser')\n",
        "        text = soup.get_text()\n",
        "        return text.strip()\n",
        "    else:\n",
        "        print(f\"Failed to fetch data from URL: {url}, status code: {response.status_code}\")\n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 5. Analysis Configuration and Functions\n",
        "Defines keywords for monitoring liquidity and critical events. Implements core analysis functions including text cleaning, liquidity forecasting, critical event detection, and decision support system."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cAP_95mDYWYn"
      },
      "outputs": [],
      "source": [
        "# Define liquidity-related keywords\n",
        "liquidity_keywords = [\n",
        "    \"liquidity\", \"cash flow\", \"debt\", \"financing\", \"credit\",\n",
        "    \"solvency\", \"funding\", \"capital\"\n",
        "]\n",
        "\n",
        "# Define critical event keywords\n",
        "critical_event_keywords = [\n",
        "    \"bankruptcy\", \"lawsuit\", \"regulatory action\", \"recall\", \"data breach\",\n",
        "    \"fraud\", \"scandal\", \"accident\", \"disaster\", \"layoffs\"\n",
        "]\n",
        "\n",
        "# Initialize aggregation variables\n",
        "all_sentiments = []\n",
        "all_liquidity_impacts = []\n",
        "all_critical_events = []\n",
        "all_decisions = []\n",
        "total_articles_analyzed = 0\n",
        "\n",
        "def clean_text(primary_text):\n",
        "    try:\n",
        "        # Format the custom prompt with the article's primary text\n",
        "        formatted_prompt = prompt_template.format(article=primary_text)\n",
        "\n",
        "        input_length = len(formatted_prompt.split())\n",
        "        calculated_max_length = int(input_length * 0.5)\n",
        "        max_length = max(50, min(150, calculated_max_length))\n",
        "\n",
        "        # Dynamically set min_length as 30% of max_length, ensuring it's reasonable\n",
        "        min_length = max(30, int(max_length * 0.3))\n",
        "\n",
        "        # Use the summarization pipeline\n",
        "        summary = summarizer(\n",
        "            formatted_prompt,\n",
        "            max_length=24,\n",
        "            min_length=1,\n",
        "            do_sample=False\n",
        "        )[0]['summary_text']\n",
        "\n",
        "        # Perform sentiment analysis on the summary\n",
        "        sentiment = sentiment_analyzer(primary_text)[0]\n",
        "\n",
        "        return summary, sentiment\n",
        "    except Exception as e:\n",
        "        print(f\"Error cleaning text: {e}\")\n",
        "        return None, None\n",
        "\n",
        "# Function to forecast liquidity impact\n",
        "def forecast_liquidity(summary, sentiment):\n",
        "    summary_lower = summary.lower()\n",
        "    liquidity_impact = \"Neutral\"\n",
        "\n",
        "    if any(keyword in summary_lower for keyword in liquidity_keywords):\n",
        "        if sentiment['label'] == 'NEGATIVE':\n",
        "            liquidity_impact = \"Negative Impact on Liquidity\"\n",
        "        elif sentiment['label'] == 'POSITIVE':\n",
        "            liquidity_impact = \"Positive Impact on Liquidity\"\n",
        "    return liquidity_impact\n",
        "\n",
        "# Function to check for critical events\n",
        "def check_critical_events(summary):\n",
        "    summary_lower = summary.lower()\n",
        "    critical_events = []\n",
        "\n",
        "    for keyword in critical_event_keywords:\n",
        "        if keyword in summary_lower:\n",
        "            critical_events.append(keyword)\n",
        "\n",
        "    return critical_events\n",
        "\n",
        "# Function for decision support\n",
        "def decision_support_system(sentiment, liquidity_impact, critical_events):\n",
        "    decisions = []\n",
        "\n",
        "    # Decision based on sentiment\n",
        "    if sentiment['label'] == 'NEGATIVE' and sentiment['score'] > 0.7:\n",
        "        decisions.append(\"Consider reducing exposure or monitoring closely due to negative sentiment.\")\n",
        "    elif sentiment['label'] == 'POSITIVE' and sentiment['score'] > 0.7:\n",
        "        decisions.append(\"Positive outlook; potential opportunity to increase exposure.\")\n",
        "\n",
        "    # Decision based on liquidity impact\n",
        "    if liquidity_impact == \"Negative Impact on Liquidity\":\n",
        "        decisions.append(\"Potential liquidity issues detected; reassess financial stability.\")\n",
        "    elif liquidity_impact == \"Positive Impact on Liquidity\":\n",
        "        decisions.append(\"Improved liquidity expected; may strengthen financial position.\")\n",
        "\n",
        "    # Decision based on critical events\n",
        "    if critical_events:\n",
        "        decisions.append(f\"Critical events detected: {', '.join(critical_events)}. Immediate action may be required.\")\n",
        "\n",
        "    if not decisions:\n",
        "        decisions.append(\"No immediate action required; maintain current position.\")\n",
        "\n",
        "    return decisions\n",
        "\n",
        "user_input = input(\"Enter your search query: \")\n",
        "\n",
        "# Define search strategies\n",
        "search_strategies = {\n",
        "    'raw_material_costs': f'{user_input} AND (\"raw material\" OR \"supply chain\" OR \"input cost\" OR \"steel prices\")',\n",
        "    'laws_and_regulations': f'{user_input} AND (\"emission laws\" OR \"regulations\" OR \"tax\" OR \"climate regulation\")',\n",
        "    'economic_factors': f'{user_input} AND (\"financial report\" OR \"earnings\" OR \"profit\" OR \"loss\")',\n",
        "    'industry_events': f'{user_input} AND (\"auto industry\" OR \"market downturn\" OR \"recession\")',\n",
        "    'climate_and_sustainability': f'{user_input} AND (\"climate change\" OR \"carbon footprint\" OR \"sustainability initiatives\" OR \"EV investments\")'\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 6. News Fetching and Analysis\n",
        "Implements the main news fetching and analysis pipeline. Processes articles from multiple sources based on different search strategies and stores results in MongoDB."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Btz94S9BYYw8"
      },
      "outputs": [],
      "source": [
        "# Calculate the time range (from 1 month ago to now)\n",
        "current_time = datetime.utcnow()\n",
        "two_weeks_ago = current_time - timedelta(weeks=2)\n",
        "time_str = two_weeks_ago.strftime('%Y-%m-%dT%H:%M:%S')\n",
        "\n",
        "# Function to fetch news articles using an open RSS feed\n",
        "def fetch_news(query):\n",
        "    rss_url = f\"https://news.google.com/rss/search?q={query}&hl=en-US&gl=US&ceid=US:en\"\n",
        "    response = requests.get(rss_url)\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.content, features=\"xml\")\n",
        "        articles = soup.findAll('item')\n",
        "        return articles\n",
        "    else:\n",
        "        print(f\"Failed to fetch news for query: {query}\")\n",
        "        return []\n",
        "\n",
        "# Main execution loop\n",
        "for strategy_name, query in search_strategies.items():\n",
        "    articles = fetch_news(query)\n",
        "\n",
        "    for article in articles:\n",
        "        # Extract relevant information from the RSS feed\n",
        "        title = article.title.text\n",
        "        link = article.link.text\n",
        "        pub_date = article.pubDate.text\n",
        "\n",
        "        # Get the text from the article's URL\n",
        "        article_text = get_text_from_url(link)\n",
        "\n",
        "        # Clean the text and get sentiment\n",
        "        cleaned_article, sentiment = clean_text(article_text)\n",
        "\n",
        "        if cleaned_article:\n",
        "            # Forecast liquidity impact\n",
        "            liquidity_impact = forecast_liquidity(title, sentiment)\n",
        "\n",
        "            # Check for critical events\n",
        "            critical_events = check_critical_events(title)\n",
        "\n",
        "            # Get decision support recommendations\n",
        "            decisions = decision_support_system(sentiment, liquidity_impact, critical_events)\n",
        "\n",
        "            # Prepare the document to insert into MongoDB (optional)\n",
        "            document = {\n",
        "                'title': title,\n",
        "                'publishedAt': pub_date,\n",
        "                'source': article.source.text if article.source else 'Unknown',\n",
        "                'cleaned_article': title,\n",
        "                'sentiment': sentiment,\n",
        "                'liquidity_impact': liquidity_impact,\n",
        "                'critical_events': critical_events,\n",
        "                'decisions': decisions\n",
        "            }\n",
        "\n",
        "            # Collect data for aggregation\n",
        "            all_sentiments.append(sentiment)\n",
        "            all_liquidity_impacts.append(liquidity_impact)\n",
        "            all_critical_events.extend(critical_events)\n",
        "            all_decisions.extend(decisions)\n",
        "            total_articles_analyzed += 1\n",
        "\n",
        "            # Optionally, insert the document into MongoDB\n",
        "            db_res = db[strategy_name].insert_one(document)\n",
        "\n",
        "            if db_res.acknowledged:\n",
        "                print(f\"Inserted article '{title}' into '{strategy_name}' collection.\")\n",
        "            else:\n",
        "                print(f\"Failed to insert article '{title}' into MongoDB.\")\n",
        "        else:\n",
        "            print(f\"Failed to clean article '{title}' for '{strategy_name}'.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 7. Results Analysis and Summary\n",
        "Generates comprehensive summary of all analyzed articles, including sentiment distribution, liquidity impacts, critical events, and decision recommendations. Stores final analysis in MongoDB."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5b28mo2E2fge"
      },
      "outputs": [],
      "source": [
        "# After processing all articles, provide overall summary\n",
        "print(\"\\n=== Overall Analysis ===\")\n",
        "\n",
        "# Sentiment Analysis\n",
        "positive_sentiments = [s for s in all_sentiments if s['label'] == 'POSITIVE']\n",
        "negative_sentiments = [s for s in all_sentiments if s['label'] == 'NEGATIVE']\n",
        "num_positive = len(positive_sentiments)\n",
        "num_negative = len(negative_sentiments)\n",
        "total_sentiments = num_positive + num_negative\n",
        "\n",
        "print(f\"Total articles analyzed: {total_articles_analyzed}\")\n",
        "print(f\"Positive sentiments: {num_positive}\")\n",
        "print(f\"Negative sentiments: {num_negative}\")\n",
        "\n",
        "# Liquidity Impact\n",
        "positive_liquidity = all_liquidity_impacts.count(\"Positive Impact on Liquidity\")\n",
        "negative_liquidity = all_liquidity_impacts.count(\"Negative Impact on Liquidity\")\n",
        "neutral_liquidity = all_liquidity_impacts.count(\"Neutral\")\n",
        "\n",
        "print(f\"\\n=== Liquidity Impact Summary ===\")\n",
        "print(f\"Positive Impact on Liquidity: {positive_liquidity}\")\n",
        "print(f\"Negative Impact on Liquidity: {negative_liquidity}\")\n",
        "print(f\"Neutral Liquidity Impact: {neutral_liquidity}\")\n",
        "\n",
        "# Critical Events\n",
        "unique_critical_events = set(all_critical_events)\n",
        "print(f\"\\n=== Critical Events Detected ===\")\n",
        "if unique_critical_events:\n",
        "    for event in unique_critical_events:\n",
        "        print(f\"- {event}\")\n",
        "else:\n",
        "    print(\"No critical events detected.\")\n",
        "\n",
        "# Decisions\n",
        "from collections import Counter\n",
        "decision_counts = Counter(all_decisions)\n",
        "print(\"\\n=== Decision Recommendations ===\")\n",
        "for decision, count in decision_counts.items():\n",
        "    print(f\"- {decision}: {count} occurrences\")\n",
        "\n",
        "# Prepare the overall analysis document\n",
        "overall_analysis = {\n",
        "    'query': user_input,\n",
        "    'timestamp': datetime.utcnow(),\n",
        "    'total_articles_analyzed': total_articles_analyzed,\n",
        "    'sentiment_summary': {\n",
        "        'positive': num_positive,\n",
        "        'negative': num_negative,\n",
        "        'total': total_sentiments\n",
        "    },\n",
        "    'liquidity_impact_summary': {\n",
        "        'positive': positive_liquidity,\n",
        "        'negative': negative_liquidity,\n",
        "        'neutral': neutral_liquidity\n",
        "    },\n",
        "    'critical_events_detected': list(unique_critical_events),\n",
        "    'decision_recommendations': dict(decision_counts)\n",
        "}\n",
        "\n",
        "# Insert the overall analysis into MongoDB\n",
        "db_res = db['overall_analysis'].insert_one(overall_analysis)\n",
        "\n",
        "if db_res.acknowledged:\n",
        "    print(\"Overall analysis successfully inserted into MongoDB.\")\n",
        "else:\n",
        "    print(\"Failed to insert overall analysis into MongoDB.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
